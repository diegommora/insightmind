# -*- coding: utf-8 -*-
"""insightmind_topicmodeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/diegommora/ce64dedf59a1a4d4e57bc1053d75f4de/insightmind_topicmodeling.ipynb

# **InsightMind - Topic Modeling Pipeline**

This data pipeline includes starts with preloaded data from Google Maps scrapped using APIFY, then follow these steps:


*   Data Processing
*   BERTopic Model
*   Representation model for Keyword extraction
*   Plain-english descriptions for topics
"""

# Libraries
import pandas as pd
import os
import numpy as np
import tqdm
import datetime

"""# Read data"""

from google.colab import drive

drive.mount('/content/drive')

# define the Google Drive path to the files to read
file_path1 = '/content/drive/My Drive/notebooks/data/reviews.csv'
file_path2 = '/content/drive/My Drive/notebooks/data/list1.csv'
file_path3 = '/content/drive/My Drive/notebooks/data/info.csv'
file_path12 = '/content/drive/My Drive/notebooks/data/reviews2.csv'
file_path22 = '/content/drive/My Drive/notebooks/data/list2.csv'
file_path32 = '/content/drive/My Drive/notebooks/data/info2.csv'
file_path13 = '/content/drive/My Drive/notebooks/data/reviews3.csv'
file_path23 = '/content/drive/My Drive/notebooks/data/list3.csv'
file_path14 = '/content/drive/My Drive/notebooks/data/reviews4.csv'
file_path24 = '/content/drive/My Drive/notebooks/data/list4.csv'

# Read the CSV files into a pandas DataFrame
reviews_df = pd.read_csv(file_path1)
list_df = pd.read_csv(file_path2)
info_df = pd.read_csv(file_path3)

reviews_df2 = pd.read_csv(file_path12)
list_df2 = pd.read_csv(file_path22)
info_df2 = pd.read_csv(file_path32)

reviews_df3 = pd.read_csv(file_path13)
list_df3 = pd.read_csv(file_path23)

reviews_df4 = pd.read_csv(file_path14)
list_df4 = pd.read_csv(file_path24)

"""# Clean and Process the data"""

reviews = reviews_df[['title','publishedAtDate','stars','text']].copy()
cs_list = list_df[['title','categoryName','address']].copy()
cs_list2 = list_df2[['title','categoryName','address']].copy()
cs_list3 = list_df3[['title','categoryName','address']].copy()
cs_list4 = list_df4[['title','categoryName','address']].copy()
info = info_df[['title','neighborhood','location/lat','location/lng']].copy()

reviews2 = reviews_df2[['title','publishedAtDate','stars','text']].copy()
reviews3 = reviews_df3[['title','publishedAtDate','stars','text']].copy()
reviews4 = reviews_df4[['title','publishedAtDate','stars','text']].copy()

# Concatenate reviews and reviews2
combined_reviews = pd.concat([reviews, reviews2], ignore_index=True)
combined_reviews = pd.concat([combined_reviews, reviews3], ignore_index=True)
combined_reviews = pd.concat([combined_reviews, reviews4], ignore_index=True)

# Concatenate list
combined_list = pd.concat([list_df, list_df2], ignore_index=True)
combined_list = pd.concat([combined_list, list_df3], ignore_index=True)
combined_list = pd.concat([combined_list, list_df4], ignore_index=True)

# Calculate the number of rows before dropping duplicates
initial_rows = combined_reviews.shape[0]

# Drop duplicate rows based on the 'text' column
combined_reviews_deduplicated = combined_reviews.drop_duplicates(subset=['publishedAtDate','text'])
combined_list_deduplicated = combined_list.drop_duplicates(subset=['title'])

# Calculate the number of rows after dropping duplicates
rows_after_deduplication = combined_reviews_deduplicated.shape[0]

# Count the number of duplicated records
duplicated_records_count = initial_rows - rows_after_deduplication

print(f"Number of duplicated records based on 'text' column: {duplicated_records_count}")

# You can now work with combined_reviews_deduplicated
reviews_def = combined_reviews_deduplicated.copy()

reviews_def = pd.merge(reviews_def, combined_list_deduplicated[['title','categoryName','location/lat','location/lng']], on='title', how='left')
#date format
reviews_def['publishedAtDate'] = pd.to_datetime(reviews_def['publishedAtDate']).dt.strftime('%Y-%m-%d')
reviews_def.dropna(subset=['text'], inplace=True)
#column names
reviews_def.rename(columns={'publishedAtDate': 'review_date', 'categoryName': 'category', 'location/lat': 'lat', 'location/lng': 'long'}, inplace=True)

pd.set_option('display.max_colwidth', None)
reviews_def[reviews_def['text']>=1000].sample(5)

star_counts = reviews_def['stars'].value_counts()
display(star_counts)



reviews_def.category.unique()

reviews_def = reviews_def[reviews_def['title'] != 'Tim Hortons']
reviews_def = reviews_def[reviews_def['title'] != 'Starbucks']
reviews_def = reviews_def[reviews_def['category'].isin(['Coffee shop','Cafe','Coffee roasters','Espresso bar','Coffee store'])]

# Count the number of unique values in the 'title' column
unique_title_count = reviews_def['title'].nunique()

print(f"Total number of unique titles: {unique_title_count}")

reviews_def['text_length'] = reviews_def['text'].str.len()
reviews_def = reviews_def[reviews_def['text_length'] >= 50].reset_index(drop=True)
reviews_def = reviews_def[reviews_def['text_length'] <= 1200].reset_index(drop=True)

import re
# Remove escape sequences like \n, \t, etc.
reviews_def['text'] = reviews_def['text'].str.replace(r'\s+', ' ', regex=True).str.strip()

# Function to remove emojis/emoticons
def remove_emojis(text):
    emoji_pattern = re.compile(
        "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002700-\U000027BF"  # dingbats
        u"\U0001F900-\U0001F9FF"  # supplemental symbols
        u"\U00002600-\U000026FF"  # miscellaneous symbols
        u"\U0001FA70-\U0001FAFF"  # extended symbols (like ðŸ«¶ðŸ½)
        u"\U00002500-\U00002BEF"  # various Chinese/Japanese characters and shapes
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

# Apply to DataFrame
reviews_def['text'] = reviews_def['text'].apply(remove_emojis)

positive_reviews = reviews_def[reviews_def['stars'].isin([4,5])].reset_index(drop=True)
negative_reviews = reviews_def[reviews_def['stars'].isin([1,2,3])].reset_index(drop=True)

print("positive reviews: ",positive_reviews.text.count(),"   Negative reviews: ",negative_reviews.text.count())

"""## BERTopic"""

# Install BERTopic and its dependencies
!pip install bertopic umap-learn hdbscan -q

from umap import UMAP
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from transformers import pipeline
from hdbscan import HDBSCAN

umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)
hdbscan_model = HDBSCAN(min_cluster_size=8, metric='euclidean', cluster_selection_method='eom', prediction_data=True)
vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words="english")
sentence_model = SentenceTransformer("all-MiniLM-L12-v2")

embeddings_p = sentence_model.encode(positive_reviews['text'], show_progress_bar=True)

embeddings_n = sentence_model.encode(negative_reviews['text'], show_progress_bar=True)

"""## Representation Model"""

from bertopic.representation import MaximalMarginalRelevance, KeyBERTInspired, PartOfSpeech
import spacy

main_representation_model = KeyBERTInspired()
pos = PartOfSpeech("en_core_web_sm", top_n_words=15)
keybert_model = KeyBERTInspired(top_n_words=15)
mmr = MaximalMarginalRelevance(diversity=0.5, top_n_words = 5)

pos_key_mmr = [pos, keybert_model, mmr]

representation_model = {
    "main": main_representation_model,
    "part_of_speech": pos,
    "keyb": keybert_model,
    "mix": pos_key_mmr
}

topic_model_p = BERTopic(
    embedding_model=sentence_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    calculate_probabilities=True,
    representation_model=representation_model
    )
topics_p, probs_p = topic_model_p.fit_transform(positive_reviews['text'], embeddings_p)

topic_model_n = BERTopic(
    embedding_model=sentence_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    calculate_probabilities=True,
    representation_model=representation_model
    )
topics_n, probs_n = topic_model_n.fit_transform(negative_reviews['text'], embeddings_n)

pd.set_option('display.max_colwidth', 150)
topic_model_n.get_topic_info()

